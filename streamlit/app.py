import streamlit as st
import sys
import os
import atexit
from dotenv import load_dotenv
import requests

# ThÃªm Ä‘Æ°á»ng dáº«n tá»›i thÆ° má»¥c gá»‘c cá»§a project
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.append(project_root)

from rag_pipeline.src import create_pipeline, DEFAULT_MODEL, DEFAULT_COLLECTION

# ÄÆ°á»ng dáº«n tuyá»‡t Ä‘á»‘i tá»›i file .env trong rag_pipeline
abs_env_path = os.path.join(project_root, 'rag_pipeline', '.env')
if os.path.exists(abs_env_path):
    load_dotenv(abs_env_path)

print("GROQ_API_KEY:", os.getenv("GROQ_API_KEY"))

# Cleanup function for session end
def cleanup_session():
    """Cleanup resources when session ends"""
    if 'rag_pipeline' in st.session_state and st.session_state.rag_pipeline:
        try:
            st.session_state.rag_pipeline.cleanup()
        except:
            pass

# Register cleanup
atexit.register(cleanup_session)

# Page config
st.set_page_config(
    page_title="ğŸ¥ Trá»£ lÃ½ Y táº¿ AI",
    page_icon="ğŸ¥",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
        display: flex;
        flex-direction: column;
    }
    .user-message {
        background-color: #f0f2f6;
        border-left: 4px solid #1f77b4;
    }
    .bot-message {
        background-color: #e8f4fd;
        border-left: 4px solid #28a745;
    }
    .source-info {
        font-size: 0.8rem;
        color: #666;
        margin-top: 0.5rem;
        padding: 0.5rem;
        background-color: #f8f9fa;
        border-radius: 0.3rem;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'messages' not in st.session_state:
    st.session_state.messages = []

if 'selected_model' not in st.session_state:
    st.session_state.selected_model = 'llama-3.3-70b-versatile'

if 'rag_pipeline' not in st.session_state:
    with st.spinner('ğŸ”„ Äang khá»Ÿi táº¡o há»‡ thá»‘ng AI...'):
        try:
            # Create pipeline with Qdrant Cloud
            st.session_state.rag_pipeline = create_pipeline(
                collection_name='medical_data',
                model_name=st.session_state.selected_model
            )
            st.success('âœ… Há»‡ thá»‘ng AI Ä‘Ã£ sáºµn sÃ ng!')
        except Exception as e:
            st.error(f'âŒ Lá»—i khá»Ÿi táº¡o: {str(e)}')
            st.error('Vui lÃ²ng kiá»ƒm tra QDRANT_CLOUD_URL vÃ  QDRANT_API_KEY trong file .env')
            st.session_state.rag_pipeline = None

# Main header
st.markdown('<h1 class="main-header">ğŸ¥ Trá»£ lÃ½ Y táº¿ AI</h1>', unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.header("â„¹ï¸ ThÃ´ng tin há»‡ thá»‘ng")
    
    # Model Selection
    st.subheader("ğŸ¤– Chá»n AI Model")
    available_models = [
        'llama-3.3-70b-versatile',
        'llama-3.1-8b-instant'
    ]
    
    model_descriptions = {
        'llama-3.3-70b-versatile': 'ğŸ¦™ LLaMA 3.3 70B (Tá»‘t nháº¥t)',
        'llama-3.1-8b-instant': 'âš¡ LLaMA 3.1 8B (Nhanh)'
    }
    
    new_model = st.selectbox(
        "Chá»n model:",
        available_models,
        index=available_models.index(st.session_state.selected_model),
        format_func=lambda x: model_descriptions.get(x, x)
    )
    
    # Handle model change
    if new_model != st.session_state.selected_model:
        with st.spinner(f'ğŸ”„ Äang chuyá»ƒn sang {new_model}...'):
            try:
                if st.session_state.rag_pipeline:
                    success = st.session_state.rag_pipeline.change_model(new_model)
                    if success:
                        st.session_state.selected_model = new_model
                        st.success(f'âœ… ÄÃ£ chuyá»ƒn sang {new_model}!')
                        st.rerun()
                    else:
                        st.error('âŒ KhÃ´ng thá»ƒ chuyá»ƒn model!')
                else:
                    # Create new pipeline with new model
                    st.session_state.rag_pipeline = create_pipeline(
                        collection_name='medical_data',
                        model_name=new_model
                    )
                    st.session_state.selected_model = new_model
                    st.success(f'âœ… ÄÃ£ khá»Ÿi táº¡o {new_model}!')
                    st.rerun()
            except Exception as e:
                st.error(f'âŒ Lá»—i chuyá»ƒn model: {str(e)}')
    
    st.markdown("---")
    
    if st.session_state.rag_pipeline:
        stats = st.session_state.rag_pipeline.get_stats()
        if stats['status'] == 'active':
            st.success("ğŸŸ¢ Äang hoáº¡t Ä‘á»™ng")
            st.metric("â˜ï¸ Káº¿t ná»‘i", stats.get('connection_type', 'Unknown'))
            st.metric("ğŸ“Š TÃ i liá»‡u y táº¿", f"{stats['vector_count']:,}")
            st.metric("ğŸ¤– AI Model", stats['llm_model'])
            st.metric("ğŸ” Embedding", stats['embedding_model'].split('/')[-1])
        else:
            st.error("ğŸ”´ Lá»—i há»‡ thá»‘ng")
            # Show detailed error information
            st.error(f"Chi tiáº¿t lá»—i: {stats}")
            if 'message' in stats:
                st.error(f"ThÃ´ng bÃ¡o: {stats['message']}")
            
            # Show available collections if any
            if 'available_collections' in stats:
                st.warning(f"Collections cÃ³ sáºµn: {', '.join(stats['available_collections'])}")
    else:
        st.error("ğŸ”´ ChÆ°a káº¿t ná»‘i")
    
    st.markdown("---")
    st.markdown("""
    ### ğŸ“‹ HÆ°á»›ng dáº«n sá»­ dá»¥ng
    1. Äáº·t cÃ¢u há»i vá» sá»©c khá»e
    2. Há»‡ thá»‘ng sáº½ tÃ¬m kiáº¿m trong database y táº¿
    3. AI sáº½ tráº£ lá»i dá»±a trÃªn thÃ´ng tin chuyÃªn mÃ´n
    
    ### âš ï¸ LÆ°u Ã½
    - ThÃ´ng tin chá»‰ mang tÃ­nh tham kháº£o
    - KhÃ´ng thay tháº¿ tÆ° váº¥n y táº¿ chuyÃªn nghiá»‡p
    - LuÃ´n tham kháº£o bÃ¡c sÄ© cho váº¥n Ä‘á» nghiÃªm trá»ng
    """)
    
    if st.button("ğŸ—‘ï¸ XÃ³a lá»‹ch sá»­ chat"):
        st.session_state.messages = []
        st.rerun()

# Main chat interface
st.markdown("### ğŸ’¬ TrÃ² chuyá»‡n vá»›i trá»£ lÃ½ y táº¿")

# Display chat messages
for message in st.session_state.messages:
    if message["role"] == "user":
        st.markdown(f"""
        <div class="chat-message user-message">
            <strong>ğŸ‘¤ Báº¡n:</strong><br>
            {message["content"]}
        </div>
        """, unsafe_allow_html=True)
    else:
        # Display bot message
        st.markdown(f"""
        <div class="chat-message bot-message">
            <strong>ğŸ¤– Trá»£ lÃ½ Y táº¿:</strong><br>
            {message["content"]}
        </div>
        """, unsafe_allow_html=True)
        
        # Display sources using Streamlit components
        if "sources" in message and message["sources"]:
            # Check if highest confidence score is >= 0.6
            max_score = max(source.get('score', 0) for source in message["sources"])
            if max_score >= 0.6:
                st.markdown("**ğŸ“š Nguá»“n tham kháº£o:**")
                sources = message["sources"]
                
                def dedup_sources(sources, min_score=0.7):
                    # Lá»c trÃ¹ng theo (title, url), giá»¯ láº¡i báº£n cÃ³ score cao nháº¥t vÃ  score >= min_score
                    unique = {}
                    for src in sources:
                        score = src.get('score', 0)
                        if score < min_score:
                            continue
                        metadata = src.get('metadata', {})
                        title = metadata.get('title', metadata.get('name', ''))
                        url = metadata.get('url', metadata.get('source', metadata.get('link', '')))
                        key = (title, url)
                        if key not in unique or score > unique[key].get('score', 0):
                            unique[key] = src
                    return list(unique.values())
                
                sources = dedup_sources(sources, min_score=0.7)
                for i, source in enumerate(sources, 1):
                    metadata = source.get('metadata', {})
                    score = source.get('score', 0)
                    url = metadata.get('url', metadata.get('source', metadata.get('link', '')))
                    title = metadata.get('title', metadata.get('name', f'TÃ i liá»‡u {i}'))
                    col1, col2 = st.columns([4, 1])
                    with col1:
                        if url:
                            st.markdown(f"ğŸ“„ [{title}]({url})")
                        else:
                            st.markdown(f"ğŸ“„ {title}")
                            content_preview = source.get('content', '')[:100] + "..." if len(source.get('content', '')) > 100 else source.get('content', '')
                            st.caption(content_preview)
                    
                    with col2:
                        st.caption(f"Äá»™ liÃªn quan: {score:.2f}")
                    
                    if i < len(sources):
                        st.divider()

# Chat input
if prompt := st.chat_input("Äáº·t cÃ¢u há»i vá» sá»©c khá»e cá»§a báº¡n..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Display user message immediately
    st.markdown(f"""
    <div class="chat-message user-message">
        <strong>ğŸ‘¤ Báº¡n:</strong><br>
        {prompt}
    </div>
    """, unsafe_allow_html=True)
    
    # Generate response
    if st.session_state.rag_pipeline:
        with st.spinner('ğŸ” Äang tÃ¬m kiáº¿m thÃ´ng tin y táº¿...'):
            try:
                # Get streaming response
                result = st.session_state.rag_pipeline.query(prompt, stream=True)
                
                # Display bot message header
                st.markdown("""
                <div class="chat-message bot-message">
                    <strong>ğŸ¤– Trá»£ lÃ½ Y táº¿:</strong><br>
                """, unsafe_allow_html=True)
                
                # Create placeholder for streaming text
                response_placeholder = st.empty()
                full_response = ""
                
                # Process streaming response
                if 'answer_stream' in result:
                    response_stream = result['answer_stream']
                    
                    # Parse SSE stream
                    for line in response_stream.iter_lines():
                        if line:
                            line = line.decode('utf-8')
                            if line.startswith('data: '):
                                data = line[6:]  # Remove 'data: ' prefix
                                if data == '[DONE]':
                                    break
                                try:
                                    import json
                                    chunk = json.loads(data)
                                    if chunk['choices'][0]['delta'].get('content'):
                                        delta = chunk['choices'][0]['delta']['content']
                                        full_response += delta
                                        response_placeholder.markdown(full_response)
                                except:
                                    continue
                else:
                    # Fallback to non-streaming
                    full_response = result.get('answer', 'KhÃ´ng cÃ³ pháº£n há»“i')
                    response_placeholder.markdown(full_response)
                
                # Close the bot message div
                st.markdown("</div>", unsafe_allow_html=True)
                
                # Add to session state
                bot_message = {
                    "role": "assistant", 
                    "content": full_response,
                    "sources": result["sources"]
                }
                st.session_state.messages.append(bot_message)
                
                # Display sources using Streamlit components
                if result["sources"]:
                    # Check if highest confidence score is >= 0.6
                    max_score = max(source.get('score', 0) for source in result["sources"])
                    if max_score >= 0.6:
                        st.markdown("**ğŸ“š Nguá»“n tham kháº£o:**")
                        sources = result["sources"]
                        
                        def dedup_sources(sources, min_score=0.7):
                            # Lá»c trÃ¹ng theo (title, url), giá»¯ láº¡i báº£n cÃ³ score cao nháº¥t vÃ  score >= min_score
                            unique = {}
                            for src in sources:
                                score = src.get('score', 0)
                                if score < min_score:
                                    continue
                                metadata = src.get('metadata', {})
                                title = metadata.get('title', metadata.get('name', ''))
                                url = metadata.get('url', metadata.get('source', metadata.get('link', '')))
                                key = (title, url)
                                if key not in unique or score > unique[key].get('score', 0):
                                    unique[key] = src
                            return list(unique.values())
                        
                        sources = dedup_sources(sources, min_score=0.7)
                        for i, source in enumerate(sources, 1):
                            metadata = source.get('metadata', {})
                            score = source.get('score', 0)
                            url = metadata.get('url', metadata.get('source', metadata.get('link', '')))
                            title = metadata.get('title', metadata.get('name', f'TÃ i liá»‡u {i}'))
                            col1, col2 = st.columns([4, 1])
                            with col1:
                                if url:
                                    st.markdown(f"ğŸ“„ [{title}]({url})")
                                else:
                                    st.markdown(f"ğŸ“„ {title}")
                                    content_preview = source.get('content', '')[:100] + "..." if len(source.get('content', '')) > 100 else source.get('content', '')
                                    st.caption(content_preview)
                            
                            with col2:
                                st.caption(f"Äá»™ liÃªn quan: {score:.2f}")
                            
                            if i < len(sources):
                                st.divider()
                
            except Exception as e:
                st.error(f"âŒ Lá»—i xá»­ lÃ½: {str(e)}")
    else:
        st.error("âŒ Há»‡ thá»‘ng chÆ°a sáºµn sÃ ng. Vui lÃ²ng lÃ m má»›i trang.")

# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666; font-size: 0.9rem;">
    ğŸ¥ Trá»£ lÃ½ Y táº¿ AI | TÃ¡c giáº£ Äá»— Quá»‘c DÅ©ng| Made with â¤ï¸ for Healthcare
</div>
""", unsafe_allow_html=True)

# Test Groq API
api_key = os.getenv("GROQ_API_KEY")
print("GROQ_API_KEY:", api_key)
response = requests.get(
    'https://api.groq.com/openai/v1/models',
    headers={'Authorization': f'Bearer {api_key}'},
    timeout=10
)
print("Status code:", response.status_code)
print("Response:", response.text) 
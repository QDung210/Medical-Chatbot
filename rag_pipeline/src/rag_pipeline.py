import logging
from typing import Dict, List, Optional
import requests
import sys
from pathlib import Path

# Add the project root to Python path
project_root = str(Path(__file__).parent.parent.parent)
if project_root not in sys.path:
    sys.path.append(project_root)

from .model_setup import ModelManager
from .vector_store import VectorStore
from .config import (
    DEFAULT_COLLECTION, DEFAULT_MODEL, DEFAULT_TOP_K,
    DEFAULT_MAX_TOKENS, EMBEDDING_MODEL
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MedicalRAGPipeline:
    def __init__(
        self,
        collection_name: str = DEFAULT_COLLECTION,
        model_name: str = DEFAULT_MODEL
    ):
        """Kh·ªüi t·∫°o Medical RAG Pipeline"""
        self.collection_name = collection_name
        self.model_name = model_name
        
        # Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn
        self.embedding_model = None
        self.vector_store = None
        self.retriever = None
        self.llm_pipeline = None
        
        self._setup_pipeline()
    
    def _setup_pipeline(self) -> None:
        """Thi·∫øt l·∫≠p c√°c th√†nh ph·∫ßn c·ªßa pipeline"""
        try:
            # 1. T·∫£i embedding model
            self.embedding_model = ModelManager.load_embedding_model()
            
            # 2. Kh·ªüi t·∫°o vector store
            self.vector_store = VectorStore()
            if not self.vector_store.client:
                raise Exception("Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Vector Store")
            
            # 3. Thi·∫øt l·∫≠p retriever
            self.retriever = self.vector_store.create_retriever(
                collection_name=self.collection_name,
                embedding_model=self.embedding_model,
                top_k=DEFAULT_TOP_K
            )
            
            # 4. T·∫£i LLM
            llm_config = ModelManager.load_llm_model(model_name=self.model_name)
            if llm_config['type'] != 'groq':
                raise Exception(f"L·ªói t·∫£i LLM: {llm_config.get('message', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')}")
            
            self.llm_pipeline = ModelManager.create_llm_pipeline(llm_config)
            
            logger.info("‚úÖ ƒê√£ thi·∫øt l·∫≠p RAG Pipeline th√†nh c√¥ng!")
            
        except Exception as e:
            logger.error(f"L·ªói thi·∫øt l·∫≠p pipeline: {e}")
            raise
    
    def _search_documents(self, query: str, limit: int = DEFAULT_TOP_K) -> List[Dict]:
        """T√¨m ki·∫øm t√†i li·ªáu li√™n quan"""
        if not self.retriever:
            return []
        return self.retriever(query, limit=limit)
    
    def _generate_context_prompt(self, query: str, documents: List[Dict]) -> str:
        """T·∫°o prompt v·ªõi context"""
        if not documents:
            context = "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan."
        else:
            context_parts = []
            for i, doc in enumerate(documents, 1):
                content = doc.get('content', '')
                score = doc.get('score', 0)
                context_parts.append(f"[T√†i li·ªáu {i}] (ƒê·ªô t∆∞∆°ng ƒë·ªìng: {score:.2f})\n{content}\n")
            context = "\n".join(context_parts)
        
        return f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω y t·∫ø th√¥ng minh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin y t·∫ø ƒë∆∞·ª£c cung c·∫•p.

TH√îNG TIN Y T·∫æ:
{context}

C√ÇU H·ªéI: {query}

H∆Ø·ªöNG D·∫™N:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
- D·ª±a v√†o th√¥ng tin ƒë∆∞·ª£c cung c·∫•p
- N·∫øu kh√¥ng c√≥ th√¥ng tin li√™n quan, h√£y n√≥i r√µ
- Lu√¥n khuy√™n b·∫°n tham kh·∫£o b√°c sƒ© cho c√°c v·∫•n ƒë·ªÅ nghi√™m tr·ªçng
- Tr·∫£ l·ªùi chi ti·∫øt v√† d·ªÖ hi·ªÉu

TR·∫¢ L·ªúI:"""
    
    def query(
        self,
        question: str,
        max_tokens: int = DEFAULT_MAX_TOKENS,
        stream: bool = False
    ) -> Dict:
        """H√†m ch√≠nh ƒë·ªÉ truy v·∫•n RAG"""
        try:
            # 1. T√¨m ki·∫øm t√†i li·ªáu
            documents = self._search_documents(question)
            
            # 2. T·∫°o prompt v·ªõi context
            prompt = self._generate_context_prompt(question, documents)
            
            # 3. Sinh c√¢u tr·∫£ l·ªùi
            response = self.llm_pipeline(prompt, max_tokens=max_tokens, stream=stream)
            
            result = {
                'question': question,
                'sources': documents,
                'context_used': len(documents) > 0
            }
            
            if stream:
                result['answer_stream'] = response
            else:
                result['answer'] = response
                
            return result
            
        except Exception as e:
            logger.error(f"L·ªói truy v·∫•n RAG: {e}")
            return {
                'question': question,
                'answer': f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}",
                'sources': [],
                'context_used': False
            }
    
    def get_stats(self) -> Dict:
        """L·∫•y th·ªëng k√™ c·ªßa pipeline"""
        try:
            if not self.vector_store.client:
                return {
                    'status': 'error',
                    'message': 'Vector Store ch∆∞a ƒë∆∞·ª£c k·∫øt n·ªëi'
                }
            
            collections = self.vector_store.client.get_collections().collections
            collection_info = None
            
            for coll in collections:
                if coll.name == self.collection_name:
                    collection_info = self.vector_store.client.get_collection(self.collection_name)
                    break
            
            if collection_info:
                return {
                    'status': 'active',
                    'collection_name': self.collection_name,
                    'vector_count': getattr(collection_info, 'points_count', 0),
                    'embedding_model': EMBEDDING_MODEL,
                    'llm_model': self.model_name,
                    'llm_provider': 'Groq API',
                    'vector_store': 'Qdrant Cloud'
                }
            
            return {
                'status': 'collection_not_found',
                'message': f'Kh√¥ng t√¨m th·∫•y collection "{self.collection_name}"',
                'available_collections': [coll.name for coll in collections]
            }
                
        except Exception as e:
            return {'status': 'error', 'message': str(e)}
    
    def cleanup(self) -> None:
        """D·ªçn d·∫πp t√†i nguy√™n"""
        if self.vector_store:
            self.vector_store.cleanup()
    
    def __del__(self):
        """H·ªßy ƒë·ªëi t∆∞·ª£ng"""
        self.cleanup()

# Singleton pattern
_global_pipeline = None

def create_pipeline(
    collection_name: str = DEFAULT_COLLECTION,
    model_name: Optional[str] = None
) -> MedicalRAGPipeline:
    """T·∫°o v√† tr·∫£ v·ªÅ instance c·ªßa RAG pipeline v·ªõi singleton pattern"""
    global _global_pipeline
    
    # Ki·ªÉm tra c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng pipeline hi·ªán t·∫°i
    if _global_pipeline is not None:
        try:
            stats = _global_pipeline.get_stats()
            if stats['status'] == 'active':
                if model_name and model_name != _global_pipeline.model_name:
                    _global_pipeline = MedicalRAGPipeline(
                        collection_name=collection_name,
                        model_name=model_name
                    )
                logger.info("‚ôªÔ∏è T√°i s·ª≠ d·ª•ng RAG pipeline")
                return _global_pipeline
            
            _global_pipeline.cleanup()
            _global_pipeline = None
        except:
            _global_pipeline = None
    
    # T·∫°o pipeline m·ªõi
    logger.info("üÜï T·∫°o RAG pipeline m·ªõi")
    _global_pipeline = MedicalRAGPipeline(
        collection_name=collection_name,
        model_name=model_name
    )
    return _global_pipeline 